#!/usr/bin/env python3
"""
Script principal para ejecutar el crawler tur√≠stico refactorizado.
"""

import os
import sys
import argparse
import logging
from pathlib import Path

# Agregar el directorio src al path para importaciones
current_dir = Path(__file__).parent
src_dir = current_dir.parent
sys.path.insert(0, str(src_dir))

try:
    import scrapy
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings
except ImportError:
    print("‚ùå Error: Scrapy no est√° instalado.")
    print("üí° Instala Scrapy con: pip install scrapy")
    sys.exit(1)

from crawler.tourist_spider import TouristSpider
from crawler.config import CITY_URLS, CITY_URLS_EXTRA


def setup_logging():
    """Configura el logging para el crawler."""
    # Configurar handler para archivo con UTF-8
    file_handler = logging.FileHandler('crawler_execution.log', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # Configurar handler para consola sin emojis
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # Formato com√∫n
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # Configurar logger root
    logging.basicConfig(
        level=logging.INFO,
        handlers=[file_handler, console_handler]
    )
    
    return logging.getLogger(__name__)


def get_available_cities():
    """Retorna la lista de ciudades disponibles."""
    all_cities = list(CITY_URLS.keys()) + list(CITY_URLS_EXTRA.keys())
    # Filtrar Madrid
    return [city for city in all_cities if city.lower() != 'madrid']


def create_scrapy_settings():
    """Crea la configuraci√≥n personalizada para Scrapy."""
    settings = {
        'BOT_NAME': 'tourist_crawler',
        'SPIDER_MODULES': ['crawler'],
        'NEWSPIDER_MODULE': 'crawler',
        
        # Configuraci√≥n de robots.txt
        'ROBOTSTXT_OBEY': False,
        
        # Configuraci√≥n de User-Agent
        'USER_AGENT': 'tourist_crawler (+http://www.yourdomain.com)',
        
        # Configuraci√≥n de delays y concurrencia
        'DOWNLOAD_DELAY': 2,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'CONCURRENT_REQUESTS': 8,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,
        
        # AutoThrottle
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 2.0,
        'AUTOTHROTTLE_MAX_DELAY': 10.0,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 2.0,
        'AUTOTHROTTLE_DEBUG': False,
        
        # Configuraci√≥n de cookies y headers
        'COOKIES_ENABLED': True,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        },
        
        # Configuraci√≥n de pipelines
        'ITEM_PIPELINES': {
            'crawler.pipelines.ChromaPipeline': 300,
        },
        
        # Configuraci√≥n de logging
        'LOG_LEVEL': 'INFO',
        'LOG_FILE': 'crawler_execution.log',
        
        # Configuraci√≥n de duraci√≥n
        'CLOSESPIDER_TIMEOUT': 3600,  # 1 hora m√°ximo
        'CLOSESPIDER_PAGECOUNT': 100,  # M√°ximo 100 p√°ginas por defecto
        
        # Configuraci√≥n de memoria
        'MEMUSAGE_ENABLED': True,
        'MEMUSAGE_LIMIT_MB': 2048,
        'MEMUSAGE_WARNING_MB': 1024,
        
        # Configuraci√≥n de duplicados
        'DUPEFILTER_CLASS': 'scrapy.dupefilters.RFPDupeFilter',
        'DUPEFILTER_DEBUG': False,
        
        # Configuraci√≥n de retry
        'RETRY_ENABLED': True,
        'RETRY_TIMES': 2,
        'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],
        
        # Configuraci√≥n de redirecciones
        'REDIRECT_ENABLED': True,
        'REDIRECT_MAX_TIMES': 3,
    }
    
    return settings


def run_crawler(target_city=None, max_pages=None):
    """
    Ejecuta el crawler con los par√°metros especificados.
    
    Args:
        target_city (str, optional): Ciudad espec√≠fica a crawlear
        max_pages (int, optional): N√∫mero m√°ximo de p√°ginas a procesar
    """
    logger = setup_logging()
    
    # Validar ciudad si se especifica
    if target_city:
        available_cities = get_available_cities()
        if target_city not in available_cities:
            logger.error(f"‚ùå Ciudad '{target_city}' no disponible.")
            logger.info(f"üí° Ciudades disponibles: {', '.join(available_cities)}")
            return False
    
    # Configurar Scrapy
    settings = create_scrapy_settings()
    
    # Ajustar l√≠mite de p√°ginas si se especifica
    if max_pages:
        settings['CLOSESPIDER_PAGECOUNT'] = max_pages
    
    # Crear proceso de crawler
    process = CrawlerProcess(settings)
    
    # Configurar argumentos del spider
    spider_kwargs = {}
    if target_city:
        spider_kwargs['target_city'] = target_city
    if max_pages:
        spider_kwargs['max_pages'] = max_pages
    
    logger.info("Iniciando crawler tur√≠stico...")
    if target_city:
        logger.info(f"Ciudad objetivo: {target_city}")
    else:
        logger.info("Procesando todas las ciudades disponibles")
    
    if max_pages:
        logger.info(f"L√≠mite de p√°ginas: {max_pages}")
    
    try:
        # Agregar spider al proceso
        process.crawl(TouristSpider, **spider_kwargs)
        
        # Iniciar el crawler
        process.start()
        
        logger.info("Crawler completado exitosamente")
        return True
        
    except Exception as e:
        logger.error(f"Error ejecutando el crawler: {e}")
        return False


def main():
    """Funci√≥n principal del script."""
    parser = argparse.ArgumentParser(
        description='Ejecuta el crawler tur√≠stico refactorizado',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python main.py                           # Crawlear todas las ciudades
  python main.py -c Barcelona              # Crawlear solo Barcelona
  python main.py -c Valencia -p 20         # Crawlear Valencia con l√≠mite de 20 p√°ginas
  python main.py -p 50                     # Crawlear todas las ciudades con l√≠mite de 50 p√°ginas
  python main.py --list-cities             # Mostrar ciudades disponibles
        """
    )
    
    parser.add_argument(
        '-c', '--city',
        type=str,
        help='Ciudad espec√≠fica a crawlear (ej: Barcelona, Valencia, Sevilla)'
    )
    
    parser.add_argument(
        '-p', '--pages',
        type=int,
        help='N√∫mero m√°ximo de p√°ginas a procesar (por defecto: configuraci√≥n del spider)'
    )
    
    parser.add_argument(
        '--list-cities',
        action='store_true',
        help='Mostrar lista de ciudades disponibles y salir'
    )
    
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='Mostrar informaci√≥n detallada de ejecuci√≥n'
    )
    
    args = parser.parse_args()
    
    # Mostrar ciudades disponibles
    if args.list_cities:
        cities = get_available_cities()
        print("Ciudades disponibles para crawlear:")
        print("=" * 50)
        for i, city in enumerate(sorted(cities), 1):
            print(f"{i:2d}. {city}")
        print(f"\nTotal: {len(cities)} ciudades")
        return
    
    # Configurar nivel de logging si es verbose
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Ejecutar crawler
    success = run_crawler(target_city=args.city, max_pages=args.pages)
    
    if success:
        print("\n¬°Crawler ejecutado exitosamente!")
        print("Revisa los logs en 'crawler_execution.log' para m√°s detalles")
        print("Los datos se han guardado en ChromaDB")
    else:
        print("\nEl crawler fall√≥. Revisa los logs para m√°s informaci√≥n.")
        sys.exit(1)


if __name__ == "__main__":
    main()